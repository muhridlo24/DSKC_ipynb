{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vz61G00wybr"
      },
      "source": [
        "# Sentiment Analysis\n",
        "Anggota Kelompok\n",
        "1. Muhammad Ridlo (13219009)\n",
        "2. Ahmad Septian Brilliantino (13219038)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im8LB9rzP8Fe"
      },
      "source": [
        "## 1. Download Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9rJHQXr2JjSI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpE5XMhaP6HY",
        "outputId": "5732807e-791a-4b0e-96a6-781bda4c30c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from google.colab import drive\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wALVyU9BqRkc"
      },
      "outputs": [],
      "source": [
        "# DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "# Matplot\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
        "from keras import utils\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from  nltk.stem import SnowballStemmer\n",
        "\n",
        "# Word2vec\n",
        "import gensim\n",
        "\n",
        "# Utility\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import Counter\n",
        "import logging\n",
        "import time\n",
        "import pickle\n",
        "import itertools\n",
        "\n",
        "# Set log\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBf5_qeTJ1hb",
        "outputId": "76f48ef0-d7fe-4099-a7c6-43839e139f68"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer \n",
        "import os, re, csv, math, codecs\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tensorflow as tf  # we use both tensorflow and pytorch (pytorch for main part) , tensorflow for tokenizer\n",
        "import pickle\n",
        "torch.manual_seed(1024);"
      ],
      "metadata": {
        "id": "g8xwPP_JGNtH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbNFGcTwg5pl"
      },
      "source": [
        "## 2. Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nLovbcJOg4qC"
      },
      "outputs": [],
      "source": [
        "# DATASET\n",
        "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "DATASET_ENCODING = \"ISO-8859-1\"\n",
        "TRAIN_SIZE = 0.8\n",
        "\n",
        "# TEXT CLENAING\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "# WORD2VEC \n",
        "W2V_SIZE = 300\n",
        "W2V_WINDOW = 7\n",
        "W2V_EPOCH = 32\n",
        "W2V_MIN_COUNT = 10\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "EPOCHS = 8\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "# SENTIMENT\n",
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
        "\n",
        "# EXPORT\n",
        "KERAS_MODEL = \"model.h5\"\n",
        "WORD2VEC_MODEL = \"model.w2v\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
        "ENCODER_MODEL = \"encoder.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "B4aKOAf3s6dM"
      },
      "outputs": [],
      "source": [
        "dataset_path='/content/gdrive/MyDrive/dskc bang/Sentiment Analysis/dataset/IMDB.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "7R5BG6Bhq1Tl"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset_path)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ZEW1YseltJV4",
        "outputId": "058f92f4-440a-461a-9468-307fc7d4a841"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  review sentiment\n",
              "0      One of the other reviewers has mentioned that ...  positive\n",
              "1      A wonderful little production. <br /><br />The...  positive\n",
              "2      I thought this was a wonderful way to spend ti...  positive\n",
              "3      Basically there's a family where a little boy ...  negative\n",
              "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "...                                                  ...       ...\n",
              "49995  I thought this movie did a down right good job...  positive\n",
              "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
              "49997  I am a Catholic taught in parochial elementary...  negative\n",
              "49998  I'm going to have to disagree with the previou...  negative\n",
              "49999  No one expects the Star Trek movies to be high...  negative\n",
              "\n",
              "[50000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6cb2c48a-a577-400d-bbf9-ea8b2afc86eb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>I thought this movie did a down right good job...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>I am a Catholic taught in parochial elementary...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>I'm going to have to disagree with the previou...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>No one expects the Star Trek movies to be high...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6cb2c48a-a577-400d-bbf9-ea8b2afc86eb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6cb2c48a-a577-400d-bbf9-ea8b2afc86eb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6cb2c48a-a577-400d-bbf9-ea8b2afc86eb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "OVKOCuBItKEE",
        "outputId": "22f21d62-8dec-4ea5-8d9c-cde6f2357e36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment  kfold\n",
              "0  I give this movie a 4 cause I'm a die hard fan...          0      0\n",
              "1  Pakeezah has a very interesting history (which...          1      0\n",
              "2  ...through the similarly minded antics of Eric...          0      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-754aedaa-9dda-4d0c-96f6-b6dd6cc18d85\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>kfold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I give this movie a 4 cause I'm a die hard fan...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pakeezah has a very interesting history (which...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>...through the similarly minded antics of Eric...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-754aedaa-9dda-4d0c-96f6-b6dd6cc18d85')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-754aedaa-9dda-4d0c-96f6-b6dd6cc18d85 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-754aedaa-9dda-4d0c-96f6-b6dd6cc18d85');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# Convert sentiment columns to numerical values\n",
        "df.sentiment = df.sentiment.apply(lambda x: 1 if x=='positive' else 0)\n",
        "## Cross validation \n",
        "# create new column \"kfold\" and assign a random value\n",
        "df['kfold'] = -1\n",
        "# Random the rows of data\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "# get label\n",
        "y = df.sentiment.values\n",
        "# initialize kfold\n",
        "kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "# fill the new values to kfold column\n",
        "for fold, (train_, valid_) in enumerate(kf.split(X=df, y=y)):\n",
        "    df.loc[valid_, 'kfold'] = fold\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load fasttext embeddings\n",
        "print('loading word embeddings...')\n",
        "fasttext_embedding = {}\n",
        "f = codecs.open('/content/gdrive/MyDrive/dskc bang/Sentiment Analysis/dataset/wiki-news-300d-1M.vec', encoding='utf-8')\n",
        "for line in tqdm(f):\n",
        "    values = line.rstrip().rsplit(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    fasttext_embedding[word] = coefs\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8POI2yG4Pwz8",
        "outputId": "badfa901-959c-4cc1-a330-8899c1d35ba0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading word embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "999995it [02:50, 5850.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Standford Glove embedding.\n",
        "glove = pd.read_csv('/content/gdrive/MyDrive/dskc bang/Sentiment Analysis/dataset/glove.6B.100d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
        "glove_embedding = {key: val.values for key, val in glove.T.items()}\n",
        "\n",
        "# Check check the dimension of this fasttext embedding version\n",
        "glove_embedding['hello'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgZpnUNeP0BJ",
        "outputId": "830d1cca-2639-4a7c-ca77-ffb03eea2a8a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "IT10swi9tZqq"
      },
      "outputs": [],
      "source": [
        "class IMDBDataset:\n",
        "    def __init__(self, reviews, targets):\n",
        "        \"\"\"\n",
        "        Argument:\n",
        "        reviews: a numpy array\n",
        "        targets: a vector array\n",
        "        \n",
        "        Return xtrain and ylabel in torch tensor datatype, stored in dictionary format\n",
        "        \"\"\"\n",
        "        self.reviews = reviews\n",
        "        self.target = targets\n",
        "    \n",
        "    def __len__(self):\n",
        "        # return length of dataset\n",
        "        return len(self.reviews)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # given an idex (item), return review and target of that index in torch tensor\n",
        "        review = torch.tensor(self.reviews[index,:], dtype = torch.long)\n",
        "        target = torch.tensor(self.target[index], dtype = torch.float)\n",
        "        \n",
        "        return {'review': review,\n",
        "                'target': target}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "gz3Db43gu94z"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, embedding_matrix):\n",
        "        \"\"\"\n",
        "        Given embedding_matrix: numpy array with vector for all words\n",
        "        return prediction ( in torch tensor format)\n",
        "        \"\"\"\n",
        "        super(LSTM, self).__init__()\n",
        "        # Number of words = number of rows in embedding matrix\n",
        "        num_words = embedding_matrix.shape[0]\n",
        "        # Dimension of embedding is num of columns in the matrix\n",
        "        embedding_dim = embedding_matrix.shape[1]\n",
        "        # Define an input embedding layer\n",
        "        self.embedding = nn.Embedding(\n",
        "                                      num_embeddings=num_words,\n",
        "                                      embedding_dim=embedding_dim)\n",
        "        # Embedding matrix actually is collection of parameter\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype = torch.float32))\n",
        "        # Because we use pretrained embedding (GLove, Fastext,etc) so we turn off requires_grad-meaning we do not train gradient on embedding weight\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        # LSTM with hidden_size = 128\n",
        "        self.lstm = nn.LSTM(\n",
        "                            embedding_dim, \n",
        "                            128,\n",
        "                            bidirectional=True,\n",
        "                            batch_first=True,\n",
        "                             )\n",
        "        # Input(512) because we use bi-directional LSTM ==> hidden_size*2 + maxpooling **2  = 128*4 = 512, will be explained more on forward method\n",
        "        self.out = nn.Linear(512, 1)\n",
        "    def forward(self, x):\n",
        "        # pass input (tokens) through embedding layer\n",
        "        x = self.embedding(x)\n",
        "        # fit embedding to LSTM\n",
        "        hidden, _ = self.lstm(x)\n",
        "        # apply mean and max pooling on lstm output\n",
        "        avg_pool= torch.mean(hidden, 1)\n",
        "        max_pool, index_max_pool = torch.max(hidden, 1)\n",
        "        # concat avg_pool and max_pool ( so we have 256 size, also because this is bidirectional ==> 256*2 = 512)\n",
        "        out = torch.cat((avg_pool, max_pool), 1)\n",
        "        # fit out to self.out to conduct dimensionality reduction from 512 to 1\n",
        "        out = self.out(out)\n",
        "        # return output\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "1awA105mvACu"
      },
      "outputs": [],
      "source": [
        "def train(data_loader, model, optimizer, device):\n",
        "    \"\"\"\n",
        "    this is model training for one epoch\n",
        "    data_loader:  this is torch dataloader, just like dataset but in torch and devide into batches\n",
        "    model : lstm\n",
        "    optimizer : torch optimizer : adam\n",
        "    device:  cuda or cpu\n",
        "    \"\"\"\n",
        "    # set model to training mode\n",
        "    model.train()\n",
        "    # go through batches of data in data loader\n",
        "    for data in data_loader:\n",
        "        reviews = data['review']\n",
        "        targets = data['target']\n",
        "        # move the data to device that we want to use\n",
        "        reviews = reviews.to(device, dtype = torch.long)\n",
        "        targets = targets.to(device, dtype = torch.float)\n",
        "        # clear the gradient\n",
        "        optimizer.zero_grad()\n",
        "        # make prediction from model\n",
        "        predictions = model(reviews)\n",
        "        # caculate the losses\n",
        "        loss = nn.BCEWithLogitsLoss()(predictions, targets.view(-1,1))\n",
        "        # backprob\n",
        "        loss.backward()\n",
        "        #single optimization step\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "3cl0wK1cvCF8"
      },
      "outputs": [],
      "source": [
        "def evaluate(data_loader, model, device):\n",
        "    final_predictions = []\n",
        "    final_targets = []\n",
        "    model.eval()\n",
        "    # turn off gradient calculation\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            reviews = data['review']\n",
        "            targets = data['target']\n",
        "            reviews = reviews.to(device, dtype = torch.long)\n",
        "            targets = targets.to(device, dtype=torch.float)\n",
        "            # make prediction\n",
        "            predictions = model(reviews)\n",
        "            # move prediction and target to cpu\n",
        "            predictions = predictions.cpu().numpy().tolist()\n",
        "            targets = data['target'].cpu().numpy().tolist()\n",
        "            # add predictions to final_prediction\n",
        "            final_predictions.extend(predictions)\n",
        "            final_targets.extend(targets)\n",
        "    return final_predictions, final_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "G51uujaNvDfg"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "CTHNIOPrD1vd"
      },
      "outputs": [],
      "source": [
        "def create_embedding_matrix(word_index, embedding_dict=None, d_model=100):\n",
        "    \"\"\"\n",
        "     this function create the embedding matrix save in numpy array\n",
        "    :param word_index: a dictionary with word: index_value\n",
        "    :param embedding_dict: a dict with word embedding\n",
        "    :d_model: the dimension of word pretrained embedding, here I just set to 100, we will define again\n",
        "    :return a numpy array with embedding vectors for all known words\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, d_model))\n",
        "    ## loop over all the words\n",
        "    for word, index in word_index.items():\n",
        "        if word in embedding_dict:\n",
        "            embedding_matrix[index] = embedding_dict[word]\n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "9ZHTSyAbD3qd"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Tokenization\n",
        "# use tf.keras for tokenization,  \n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(df.review.values.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "i79odKdvq0F3"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(tokenizer,open('tokenizer.pkl','wb'))"
      ],
      "metadata": {
        "id": "QkzLTjiMq2Hz"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "tZGCHxbQJftG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6df2cc3-98ed-4802-c8fa-ed7f9bbb96ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load fasttext embedding\n",
            "training model\n",
            "FOLD:0, epoch: 0, accuracy_score: 0.8678\n",
            "FOLD:0, epoch: 1, accuracy_score: 0.8772\n",
            "FOLD:0, epoch: 2, accuracy_score: 0.8831\n",
            "FOLD:0, epoch: 3, accuracy_score: 0.8831\n",
            "FOLD:0, epoch: 4, accuracy_score: 0.8852\n"
          ]
        }
      ],
      "source": [
        "print('Load fasttext embedding')\n",
        "embedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict=fasttext_embedding, d_model=300)\n",
        "\n",
        "# I just run 1 fold to reduce the time. You can try more fold to get better generalization\n",
        "for fold in range(1):\n",
        "    # STEP 2: cross validation\n",
        "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
        "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
        "    \n",
        "    # STEP 3: pad sequence\n",
        "    xtrain = tokenizer.texts_to_sequences(train_df.review.values)\n",
        "    xtest = tokenizer.texts_to_sequences(valid_df.review.values)\n",
        "    \n",
        "    # zero padding\n",
        "    xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain, maxlen=MAX_LEN)\n",
        "    xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n",
        "    \n",
        "    # STEP 4: initialize dataset class for training\n",
        "    train_dataset = IMDBDataset(reviews=xtrain, targets=train_df.sentiment.values)\n",
        "    \n",
        "    # STEP 5: Load dataset to Pytorch DataLoader\n",
        "    # after we have train_dataset, we create a torch dataloader to load train_dataset class based on specified batch_size\n",
        "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE, num_workers=2)\n",
        "    # initialize dataset class for validation\n",
        "    valid_dataset = IMDBDataset(reviews=xtest, targets=valid_df.sentiment.values)\n",
        "    valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = VALID_BATCH_SIZE, num_workers=1)\n",
        "    \n",
        "    # STEP 6: Running \n",
        "    device = torch.device('cpu')\n",
        "    # feed embedding matrix to lstm\n",
        "    model_fasttext = LSTM(embedding_matrix)\n",
        "    # set model to cuda device\n",
        "    model_fasttext.to(device)\n",
        "    # initialize Adam optimizer\n",
        "    model_fasttext = LSTM(embedding_matrix)\n",
        "    optimizer = torch.optim.Adam(model_fasttext.parameters(), lr=1e-3)\n",
        "    \n",
        "    print('training model')\n",
        "   \n",
        "    for epoch in range(EPOCHS):\n",
        "        #train one epoch\n",
        "        train(train_data_loader, model_fasttext, optimizer, device)\n",
        "        #validate\n",
        "        outputs, targets = evaluate(valid_data_loader, model_fasttext, device)\n",
        "        # threshold\n",
        "        outputs = np.array(outputs) >= 0.5\n",
        "        # calculate accuracy\n",
        "        accuracy = metrics.accuracy_score(targets, outputs)\n",
        "        print(f'FOLD:{fold}, epoch: {epoch}, accuracy_score: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "l6IR8yHhiXAk"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMiPwHofD6Eo"
      },
      "outputs": [],
      "source": [
        "print('Load Glove embedding')\n",
        "embedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict=glove_embedding, d_model=100)\n",
        "\n",
        "for fold in range(1):\n",
        "    # STEP 2: cross validation\n",
        "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
        "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
        "    \n",
        "    # STEP 3: pad sequence\n",
        "    xtrain = tokenizer.texts_to_sequences(train_df.review.values)\n",
        "    xtest = tokenizer.texts_to_sequences(valid_df.review.values)\n",
        "    \n",
        "    # zero padding\n",
        "    xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain, maxlen=MAX_LEN)\n",
        "    xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n",
        "    \n",
        "    # STEP 4: initialize dataset class for training\n",
        "    train_dataset = IMDBDataset(reviews=xtrain, targets=train_df.sentiment.values)\n",
        "    \n",
        "    # STEP 5: Load dataset to Pytorch DataLoader\n",
        "    # after we have train_dataset, we create a torch dataloader to load train_dataset class based on specified batch_size\n",
        "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE, num_workers=2)\n",
        "    # initialize dataset class for validation\n",
        "    valid_dataset = IMDBDataset(reviews=xtest, targets=valid_df.sentiment.values)\n",
        "    valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = VALID_BATCH_SIZE, num_workers=1)\n",
        "    \n",
        "    # STEP 6: Running \n",
        "    device = torch.device('cpu')\n",
        "    # feed embedding matrix to lstm\n",
        "    model_glove = LSTM(embedding_matrix)\n",
        "    # set model to cuda device\n",
        "    model_glove.to(device)\n",
        "    # initialize Adam optimizer\n",
        "    optimizer = torch.optim.Adam(model_glove.parameters(), lr=1e-3)\n",
        "    \n",
        "    print('training model')\n",
        "   \n",
        "    for epoch in range(EPOCHS):\n",
        "        #train one epoch\n",
        "        train(train_data_loader, model_glove, optimizer, device)\n",
        "        #validate\n",
        "        outputs, targets = evaluate(valid_data_loader, model_glove, device)\n",
        "        # threshold\n",
        "        outputs = np.array(outputs) >= 0.5\n",
        "        # calculate accuracy\n",
        "        accuracy = metrics.accuracy_score(targets, outputs)\n",
        "        print(f'FOLD:{fold}, epoch: {epoch}, accuracy_score: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mytwtsJiuEh",
        "outputId": "71843677-0b2b-40cd-8696-6518fc8c07e0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (embedding): Embedding(124253, 300)\n",
              "  (lstm): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
              "  (out): Linear(in_features=512, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "gRE2qUr-YGVg"
      },
      "outputs": [],
      "source": [
        "def Interact_user_input(model):\n",
        "    '''\n",
        "    model: trained model : fasttext model or glove model\n",
        "    '''\n",
        "    model.eval()\n",
        "    \n",
        "    sentence = ''\n",
        "    while True:\n",
        "        try:\n",
        "            sentence = input('Review: ')\n",
        "            if sentence in ['q','quit']: \n",
        "                break\n",
        "            sentence = np.array([sentence])\n",
        "            sentence_token = tokenizer.texts_to_sequences(sentence)\n",
        "            sentence_token = tf.keras.preprocessing.sequence.pad_sequences(sentence_token, maxlen = MAX_LEN)\n",
        "            sentence_train = torch.tensor(sentence_token, dtype = torch.long).to(device, dtype = torch.long)\n",
        "            predict = model(sentence_train)\n",
        "            if predict.item() > 0.5:\n",
        "                print('------> Positive')\n",
        "            else:\n",
        "                print('------> Negative')\n",
        "        except KeyError:\n",
        "            print('please enter again')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "yJuu6d95FHu7",
        "outputId": "a3316bf5-b3e8-412b-ebd3-528d90840b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: the movie was bad\n",
            "------> Negative\n",
            "Review: the movie was gooodd\n",
            "------> Negative\n",
            "Review: the movie was good\n",
            "------> Positive\n",
            "Review: good\n",
            "------> Positive\n",
            "Review: the movie was a little bit bad but overall is really good\n",
            "------> Negative\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-41c0b79c0748>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mInteract_user_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fasttext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-f7e640663e02>\u001b[0m in \u001b[0;36mInteract_user_input\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Review: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'quit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "Interact_user_input(model_fasttext)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save"
      ],
      "metadata": {
        "id": "2HuTQbBIp2yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.onnx.export(model_fasttext, xtest.tolist(), 'iris.onnx', input_names=[\"features\"], output_names=[\"logits\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "2uL9SxVSojfp",
        "outputId": "86befacd-3f77-47e9-8a34-343d1e2d128a"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-5f2df7a250c8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fasttext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iris.onnx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \"\"\"\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m     _export(\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1546\u001b[0m             \u001b[0m_validate_dynamic_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1548\u001b[0;31m             graph, params_dict, torch_out = _model_to_graph(\n\u001b[0m\u001b[1;32m   1549\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pre_trace_quant_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_jit_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m     \u001b[0mparams_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_named_param_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_trace_and_get_graph_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m     \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_onnx_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0mprev_autocast_cache_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_autocast_cache_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_autocast_cache_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m     trace_graph, torch_out, inputs_states = torch.jit._get_trace_graph(\n\u001b[0m\u001b[1;32m    894\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1266\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mONNXTracedModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         graph, out = torch._C._create_graph_by_tracing(\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0min_vars\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             ret_inputs.append(\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             )\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             ret_inputs.append(\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             )\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-4n1LYRZqX27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_fasttext.state_dict(),\"dskc.pkl\")"
      ],
      "metadata": {
        "id": "IJPt8iTOJPSb"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1toqHOcYtPrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fix untuk Production"
      ],
      "metadata": {
        "id": "NFEEyASftV-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load fasttext embeddings\n",
        "print('loading word embeddings...')\n",
        "fasttext_embedding = {}\n",
        "f = codecs.open('/content/gdrive/MyDrive/dskc bang/Sentiment Analysis/dataset/wiki-news-300d-1M.vec', encoding='utf-8')\n",
        "for line in tqdm(f):\n",
        "    values = line.rstrip().rsplit(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    fasttext_embedding[word] = coefs\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNklpgNbtXqI",
        "outputId": "dbd1d227-c9f2-4774-b186-9f7c059a0d27"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading word embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "999995it [02:32, 6557.53it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x4k8jt83thuK"
      },
      "outputs": [],
      "source": [
        "class IMDBDataset:\n",
        "    def __init__(self, reviews, targets):\n",
        "        \"\"\"\n",
        "        Argument:\n",
        "        reviews: a numpy array\n",
        "        targets: a vector array\n",
        "        \n",
        "        Return xtrain and ylabel in torch tensor datatype, stored in dictionary format\n",
        "        \"\"\"\n",
        "        self.reviews = reviews\n",
        "        self.target = targets\n",
        "    \n",
        "    def __len__(self):\n",
        "        # return length of dataset\n",
        "        return len(self.reviews)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # given an idex (item), return review and target of that index in torch tensor\n",
        "        review = torch.tensor(self.reviews[index,:], dtype = torch.long)\n",
        "        target = torch.tensor(self.target[index], dtype = torch.float)\n",
        "        \n",
        "        return {'review': review,\n",
        "                'target': target}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cxKnirFBthuL"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, embedding_matrix):\n",
        "        \"\"\"\n",
        "        Given embedding_matrix: numpy array with vector for all words\n",
        "        return prediction ( in torch tensor format)\n",
        "        \"\"\"\n",
        "        super(LSTM, self).__init__()\n",
        "        # Number of words = number of rows in embedding matrix\n",
        "        num_words = embedding_matrix.shape[0]\n",
        "        # Dimension of embedding is num of columns in the matrix\n",
        "        embedding_dim = embedding_matrix.shape[1]\n",
        "        # Define an input embedding layer\n",
        "        self.embedding = nn.Embedding(\n",
        "                                      num_embeddings=num_words,\n",
        "                                      embedding_dim=embedding_dim)\n",
        "        # Embedding matrix actually is collection of parameter\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype = torch.float32))\n",
        "        # Because we use pretrained embedding (GLove, Fastext,etc) so we turn off requires_grad-meaning we do not train gradient on embedding weight\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        # LSTM with hidden_size = 128\n",
        "        self.lstm = nn.LSTM(\n",
        "                            embedding_dim, \n",
        "                            128,\n",
        "                            bidirectional=True,\n",
        "                            batch_first=True,\n",
        "                             )\n",
        "        # Input(512) because we use bi-directional LSTM ==> hidden_size*2 + maxpooling **2  = 128*4 = 512, will be explained more on forward method\n",
        "        self.out = nn.Linear(512, 1)\n",
        "    def forward(self, x):\n",
        "        # pass input (tokens) through embedding layer\n",
        "        x = self.embedding(x)\n",
        "        # fit embedding to LSTM\n",
        "        hidden, _ = self.lstm(x)\n",
        "        # apply mean and max pooling on lstm output\n",
        "        avg_pool= torch.mean(hidden, 1)\n",
        "        max_pool, index_max_pool = torch.max(hidden, 1)\n",
        "        # concat avg_pool and max_pool ( so we have 256 size, also because this is bidirectional ==> 256*2 = 512)\n",
        "        out = torch.cat((avg_pool, max_pool), 1)\n",
        "        # fit out to self.out to conduct dimensionality reduction from 512 to 1\n",
        "        out = self.out(out)\n",
        "        # return output\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DB7UuJ7cthuL"
      },
      "outputs": [],
      "source": [
        "def train(data_loader, model, optimizer, device):\n",
        "    \"\"\"\n",
        "    this is model training for one epoch\n",
        "    data_loader:  this is torch dataloader, just like dataset but in torch and devide into batches\n",
        "    model : lstm\n",
        "    optimizer : torch optimizer : adam\n",
        "    device:  cuda or cpu\n",
        "    \"\"\"\n",
        "    # set model to training mode\n",
        "    model.train()\n",
        "    # go through batches of data in data loader\n",
        "    for data in data_loader:\n",
        "        reviews = data['review']\n",
        "        targets = data['target']\n",
        "        # move the data to device that we want to use\n",
        "        reviews = reviews.to(device, dtype = torch.long)\n",
        "        targets = targets.to(device, dtype = torch.float)\n",
        "        # clear the gradient\n",
        "        optimizer.zero_grad()\n",
        "        # make prediction from model\n",
        "        predictions = model(reviews)\n",
        "        # caculate the losses\n",
        "        loss = nn.BCEWithLogitsLoss()(predictions, targets.view(-1,1))\n",
        "        # backprob\n",
        "        loss.backward()\n",
        "        #single optimization step\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "t-UhNuqCthuL"
      },
      "outputs": [],
      "source": [
        "def evaluate(data_loader, model, device):\n",
        "    final_predictions = []\n",
        "    final_targets = []\n",
        "    model.eval()\n",
        "    # turn off gradient calculation\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            reviews = data['review']\n",
        "            targets = data['target']\n",
        "            reviews = reviews.to(device, dtype = torch.long)\n",
        "            targets = targets.to(device, dtype=torch.float)\n",
        "            # make prediction\n",
        "            predictions = model(reviews)\n",
        "            # move prediction and target to cpu\n",
        "            predictions = predictions.cpu().numpy().tolist()\n",
        "            targets = data['target'].cpu().numpy().tolist()\n",
        "            # add predictions to final_prediction\n",
        "            final_predictions.extend(predictions)\n",
        "            final_targets.extend(targets)\n",
        "    return final_predictions, final_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "045Wz17vthuL"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "J_Hygm0sthuM"
      },
      "outputs": [],
      "source": [
        "def create_embedding_matrix(word_index, embedding_dict=None, d_model=100):\n",
        "    \"\"\"\n",
        "     this function create the embedding matrix save in numpy array\n",
        "    :param word_index: a dictionary with word: index_value\n",
        "    :param embedding_dict: a dict with word embedding\n",
        "    :d_model: the dimension of word pretrained embedding, here I just set to 100, we will define again\n",
        "    :return a numpy array with embedding vectors for all known words\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, d_model))\n",
        "    ## loop over all the words\n",
        "    for word, index in word_index.items():\n",
        "        if word in embedding_dict:\n",
        "            embedding_matrix[index] = embedding_dict[word]\n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcT3ZaZkthuM"
      },
      "outputs": [],
      "source": [
        "# # STEP 1: Tokenization\n",
        "# # use tf.keras for tokenization,  \n",
        "# tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "# tokenizer.fit_on_texts(df.review.values.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "8X4gXuYwtYaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Interact_user_input():\n",
        "    '''\n",
        "    model: trained model : fasttext model or glove model\n",
        "    '''\n",
        "    tokenizer=pickle.load(open('/content/gdrive/MyDrive/dskc bang/Sentiment Analysis/tokenizer.pkl','rb'))\n",
        "    device=torch.device(\"cpu\")\n",
        "    embedding_matrix = create_embedding_matrix(tokenizer.word_index, embedding_dict=fasttext_embedding, d_model=300)\n",
        "    model = LSTM(embedding_matrix)\n",
        "    tor=torch.load('/content/gdrive/MyDrive/dskc bang/Sentiment Analysis/dskc.pkl')\n",
        "    model.load_state_dict(tor)\n",
        "    print(model)\n",
        "    # model.load_state_dict(torched,strict=False)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    sentence = ''\n",
        "    while True:\n",
        "        try:\n",
        "            sentence = input('Review: ')\n",
        "            if sentence in ['q','quit']: \n",
        "                break\n",
        "            tm=time.time()\n",
        "            sentence = np.array([sentence])\n",
        "            sentence_token = tokenizer.texts_to_sequences(sentence)\n",
        "            sentence_token = tf.keras.preprocessing.sequence.pad_sequences(sentence_token, maxlen = MAX_LEN)\n",
        "            sentence_train = torch.tensor(sentence_token, dtype = torch.long).to(device, dtype = torch.long)\n",
        "            predict = model(sentence_train)\n",
        "            print('predict: ',predict)\n",
        "            if predict.item() > 0.5:\n",
        "                print('------> Positive')\n",
        "            else:\n",
        "                print('------> Negative')\n",
        "            print(\"execution time: \",time.time()-tm)\n",
        "        except KeyError:\n",
        "            print('please enter again')"
      ],
      "metadata": {
        "id": "8pgfrgqPmxDX"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Interact_user_input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE6_Xm3vsrjk",
        "outputId": "a2d5e893-7a0a-492d-f857-1cbe12f227f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM(\n",
            "  (embedding): Embedding(124253, 300)\n",
            "  (lstm): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
            "  (out): Linear(in_features=512, out_features=1, bias=True)\n",
            ")\n",
            "Review: the movie is really good\n",
            "predict:  tensor([[1.0728]], grad_fn=<AddmmBackward0>)\n",
            "------> Positive\n",
            "execution time:  0.013091325759887695\n",
            "Review: the movie is bad, 10/10\n",
            "predict:  tensor([[0.0278]], grad_fn=<AddmmBackward0>)\n",
            "------> Negative\n",
            "execution time:  0.011668205261230469\n",
            "Review: the movie is not bad at all, 10/10\n",
            "predict:  tensor([[3.0738]], grad_fn=<AddmmBackward0>)\n",
            "------> Positive\n",
            "execution time:  0.010289669036865234\n",
            "Review: the movie is good, 8/10\n",
            "predict:  tensor([[7.7303]], grad_fn=<AddmmBackward0>)\n",
            "------> Positive\n",
            "execution time:  0.014666318893432617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "6GUokPRKstAa",
        "outputId": "f5de2a10-b230-4c99-b395-281febaff797"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-1f8a688cae5d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}